{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3dbb1c-55e5-418a-8dcc-75a3c7f105bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports \n",
    "\n",
    "library(tidyverse)\n",
    "library(repr)\n",
    "library(tidymodels)\n",
    "library(GGally)\n",
    "options(repr.matrix.max.rows = 6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fc9298-605f-4928-8b24-5289b78be4b5",
   "metadata": {},
   "source": [
    "## Introduction \n",
    "\n",
    "This project is part of a larger research effort led by the UBC Computer Science group studying how people play video games. The team, led by Frank Wood, operates a dedicated Minecraft server where players’ in-game actions are recorded as they move through the world. Managing this type of research environment is challenging, as the group must strategically recruit players and ensure they have enough resources, such as software licenses and server capacity, to support active gameplay. To guide these decisions, the stakeholders have outlined three major areas of interest: identifying which player characteristics predict newsletter subscription, determining which types of players are most likely to contribute large amounts of data, and forecasting peak periods of simultaneous gameplay to plan hardware allocation. My project focuses on one of these broad questions and develops a more specific, data-driven question using variables from the dataset in order to provide insights that support the research group’s operational and recruitment strategies.\n",
    "\n",
    "### Question \n",
    "\n",
    "For my project, I have decided to answer the broad question:  \n",
    "**\"What player characteristics and behaviours are most predictive of subscribing to a game-related newsletter, and how do these features differ between various player types?\"**\n",
    "\n",
    "The specific question that I have formulated is:  \n",
    "**Can a player’s experience level and age be used to predict whether they will subscribe to the game’s newsletter?**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adce1b9c-c124-433c-b366-dddda3f1c6d4",
   "metadata": {},
   "source": [
    "### Summary of methods used to perform analysis\n",
    "\n",
    "For our analysis, we began by preprocessing the `players.csv` dataset, removing unnecessary features such as `gender`, `name`, `hashedEmail`, and `played_hours`. The dataset was then split into a training set containing 75% of the data and a test set containing the remaining 25%. To ensure reproducibility, we set a random seed before specifying our model. We chose K-Nearest Neighbours (KNN) as our model since the task was a classification problem. During the data split, we stratified by the target variable `subscribe`. All predictor columns in the KNN recipe were standardized so that each variable contributed equally to the distance calculations. We then performed 5-fold cross-validation on the training set to tune the number of neighbours(K) in the model. Finally, we used the best-performing K to fit a final model and made predictions on the test set and measured the recall and accuracy of our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c5e504e-471b-483e-8876-25ad318b1374",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2318667592.py, line 11)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mplayers$experience = factor(players$experience, levels = c(\"Amateur\", \"Beginner\", \"Regular\", \"Pro\", \"Veteran\"), ordered = TRUE)\u001b[39m\n           ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# ----------- loading the data in -------------\n",
    "players = read_csv('data/players.csv')\n",
    "\n",
    "\n",
    "# ------------- Data cleaning --------------\n",
    "\n",
    "# only selecting columns that are needed\n",
    "players = select(players, Age, experience, subscribe)\n",
    "\n",
    "# Encoding the experience column\n",
    "players$experience = factor(players$experience, levels = c(\"Amateur\", \"Beginner\", \"Regular\", \"Pro\", \"Veteran\"), ordered = TRUE)\n",
    "\n",
    "# Converting target to factor type and dropping rows with missing values\n",
    "players = players |> \n",
    "        mutate(subscribe = factor(subscribe)) |>\n",
    "        drop_na()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e09bc7-7fcd-48c3-b5bd-cf2bb40e6bf3",
   "metadata": {},
   "source": [
    "**Data Wrangling**\n",
    "- Above we have selected the columns that are relevant for our analysis (`Age`, `experience` and `subscribe`). \n",
    "- We used ordinal encoding on the experience column of the dataset since it is a categorical variable with an order. We set the order of the categories as **\"Amateur\", \"Beginner\", \"Regular\", \"Pro\", \"Veteran\"**\n",
    "- We also converted `subscribe` (target variable) to a factor, as this is a classification problem.\n",
    "- There were also missing values in the data, which we decided to drop.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b7e69b-c50b-4f4e-a83c-d952024e1adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "options(repr.plot.width = 8, repr.plot.height = 6)\n",
    "# Setting the seed for reproducibility.\n",
    "set.seed(3456) \n",
    "\n",
    "# Splitting the data into a 75% training split and a 25% test split \n",
    "players_split <- initial_split(players, prop = 0.75, strata = subscribe)  \n",
    "players_train <- training(players_split)   \n",
    "players_test <- testing(players_split)\n",
    "\n",
    "players_numerical <- players_train |>\n",
    "  mutate(\n",
    "    Age = as.numeric(Age),\n",
    "    experience = as.numeric(experience),\n",
    "    subscribe = as.numeric(subscribe)\n",
    "  )\n",
    "\n",
    "\n",
    "# Visualizing of the training data\n",
    "data_viz <- ggpairs(players_numerical) +\n",
    "  ggtitle(\"Fig. 1.0: Correlations between age, experience and subscription status\")\n",
    "\n",
    "data_viz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a89fe2-3a3c-4a20-b234-430dff5ce3d7",
   "metadata": {},
   "source": [
    "From the visualization above, it is evident that : \n",
    "- There is more data for people within the age range of 20-30, and there is very little data for people who are 30+.\n",
    "- There is a lot of data for people whose experience level is Amateur (encoded as 1), and there is little data for people whose experience level is Pro (encoded as 4).\n",
    "- There are a lot of FALSE `subscribe` values (encoded as 2), but not a lot of true `subscribe` values (encoded as 1)\n",
    "- There is a low correlation between subscription and both predictor variables (Age and experience). This indicates that `Age` and `experience` alone may be poor predictors of the target; however, in combination, they could identify a signal.\n",
    "- Overall, the data has a large imbalance across all columns, and we anticipate that this will affect the model's results.\n",
    "- Since there is a large class imbalance in our target variable (`subscribe`), it is likely that we will get a high accuracy and a poor recall score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae667ce-64db-438b-b0ce-6284d9e51afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a recipe and scaling + centering data\n",
    "players_recipe <- recipe(subscribe ~ ., data = players_train) |>\n",
    "    step_scale(all_numeric_predictors()) |>\n",
    "    step_center(all_numeric_predictors()) |>\n",
    "    step_dummy(all_nominal_predictors())  # converting predictors to numerical values\n",
    "\n",
    "\n",
    "# Cross-validation and model tuning\n",
    "number_vfold  = vfold_cv(players_train, v = 5, strata = subscribe) \n",
    "\n",
    "# Testing values of K from 1 to 12\n",
    "k_vals <- tibble(neighbors = seq(from = 1, to = 11, by = 1))\n",
    "\n",
    "knn_tune = nearest_neighbor(weight_func = \"rectangular\", neighbors = tune()) |>\n",
    "      set_engine(\"kknn\") |>\n",
    "      set_mode(\"classification\")\n",
    "\n",
    "\n",
    "# Making the workflow\n",
    "knn_results <- workflow() |> \n",
    "       add_recipe(players_recipe) |> \n",
    "       add_model(knn_tune) |>  \n",
    "       tune_grid(resamples = number_vfold, grid = k_vals) |>\n",
    "       collect_metrics() |>\n",
    "       filter(.metric == \"accuracy\")\n",
    "     \n",
    "\n",
    "# Making a plot of accuracy against K score\n",
    "cross_val_plot = knn_results |>\n",
    "        ggplot(aes(x=neighbors ,y=mean)) +\n",
    "        geom_point() +\n",
    "        geom_line() + \n",
    "        labs(x = \"Number of Neighbors\" , y = \"Accuracy Estimate\") + \n",
    "        ggtitle(\"Fig 2.0: Plot of accuracy against K (nearest neigbors)\") + \n",
    "        scale_x_continuous(breaks = seq(0, 14, by = 1)) +\n",
    "        scale_y_continuous(limits = c(0.1, 1.0))\n",
    "\n",
    "\n",
    "cross_val_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa95342-55e9-42c6-b0ce-35ac77e54816",
   "metadata": {},
   "source": [
    "The code above performs a K-Nearest Neighbours (KNN) classification analysis on the `players_train` dataset. First, we created a recipe where all numeric predictors are scaled and centred, and categorical predictors are converted to dummy variables. Next, we applied 5-fold cross-validation, stratifying by the target variable `subscribe`. We specified a grid of K values from 1 to 11 to test different numbers of neighbours for the KNN model and created a workflow to tune our model. We collected the results and filtered to only retain the accuracy metric. Finally, we created a plot to display the mean accuracy against the number of neighbours, to show how model performance varies with different K values. \n",
    "\n",
    "From the plot, it is evident that the value of K, which gives the highest accuracy estimate, is 11. The Accuracy estimate is around 0.75, which is not a great score. Some potential reasons for this could be: \n",
    "- The predictor variables did not have a high correlation with the target.\n",
    "- The `Age` variable did not have a wide range of data and was also in continuous form. Discretizing `Age` could further improve model performance because patterns may be more easily uncovered in age categories.\n",
    "- The dataset is not large.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f89700-2991-471b-926f-fed483818814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a new model with our best value of K\n",
    "knn_spec <- nearest_neighbor(weight_func = \"rectangular\", neighbors = 11) |>\n",
    "       set_engine(\"kknn\") |>\n",
    "       set_mode(\"classification\")\n",
    "\n",
    "# making a workflow for the best model\n",
    "players_fit <- workflow() |>\n",
    "       add_recipe(players_recipe) |>\n",
    "       add_model(knn_spec) |>\n",
    "       fit(data = players_train)\n",
    "\n",
    "#predicting on our test set\n",
    "players_test_predictions <- predict(players_fit, players_test) |>\n",
    "  bind_cols(players_test)\n",
    "\n",
    "\n",
    "# getting the accuracy of the predictions on our test set\n",
    "players_prediction_accuracy <- players_test_predictions |>\n",
    "         metrics(truth = subscribe, estimate = .pred_class) |>\n",
    "         filter(.metric == \"accuracy\")\n",
    "\n",
    "players_prediction_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48de886b-0da0-4cc9-b428-914ee2ec2974",
   "metadata": {},
   "source": [
    "We can see above that our test set had an accuracy of around 0.71. That is close to the score that we got on our training set. The accuracy score on the test set is also not great. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3280260e-6c1a-465a-b75b-17f0da45ad29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting a confusion matrix to see what the recall and precision looks like\n",
    "players_mat <- players_test_predictions |> \n",
    "       conf_mat(truth = subscribe, estimate = .pred_class)\n",
    "\n",
    "players_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c4173e-18c1-490d-898f-64270be0d61d",
   "metadata": {},
   "source": [
    "From the confusion matrix above we can see that: \n",
    "\n",
    "We have: \n",
    "- 1 true Negative\n",
    "- 2 false Negatives\n",
    "- 12 false Positives\n",
    "- 34 true Positives\n",
    "\n",
    "This indicates that our model is predicting `TRUE` a lot more than it is predicting false and we have a high recall score.\n",
    "We can calculate recall using the formula `TP/(TP + FN)` which would be `(34)/(34 + 2) = 0.944`.\n",
    "\n",
    "That is a very high recall score! Since we are more interested in predicting the `TRUE` class, that can be good. However, the model is very bad at predicting the `FALSE` class. \n",
    "\n",
    "We expected this behaviour since there was a significant class imbalance in our target variable. If we wanted to optimize our accuracy and focus on predicting the `FALSE` class correctly, too, we could balance the class weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88bbe22f-1b3c-4001-95a1-7b771260e8bf",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "### Findings\n",
    "The KNN model achieved moderate accuracy on both the training `(≈0.75)` and test sets `(≈0.71)`. This reflects what we found in `Fig. 1.0`, where we noticed a huge imbalance in the data skewing toward players of ages 20-30 and amateur skill level. Upon calculating the recall for our model `(≈0.944)`, we found that it was indeed useful in predicting for the TRUE class but poor in predicting for `FALSE`.\n",
    "\n",
    "We anticipated that the model may struggle with classification given the shortage of observations in diverse age groups and skill levels. Another factor that made us skeptical of our model’s performance is the low rate of subscription in general. Additionally, the predictors had low correlation with the target. Our results confirmed our expectations.\n",
    "\n",
    "### Significance\n",
    "\n",
    "Our findings clearly highlight the limitations of the dataset and our approach. The model is highly biased toward predicting subscribers. If used in the real world, this could lead to misdirected decisions. The imbalance that we found suggests that age and experience are not sufficient for accurately predicting subscription. These variables, due to general demographics of individuals interested in gaming, would not provide a great help in predicting subscription as the predictor variables tend not to actually vary enough. Better balancing techniques and potentially more advanced models may be needed. Ultimately, our findings demonstrate the importance of data quality and relevance to model reliability.\n",
    "\n",
    "### Potential future questions\n",
    "\n",
    "Our findings give insight into possible more helpful future questions. Given that our variables are not strongly correlated, we can bring in another variable found in the dataset, which is session time. We may first ask whether age, experience level, and session time are correlated with one another. Then, we may investigate their ability to predict subscription. For example, we can examine each variable against subscription alone, then combine two or three of them and consolidate our findings for the highest accuracy. We may ask whether age, experience, and session time can predict subscription.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26f00e5-acac-4421-8096-4bf7c22040c1",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "- Greg Martin. (2015, September 3). Factor into numeric in R [Online forum post]. Stack Overflow. https://stackoverflow.com/questions/32370731/factor-into-numeric-in-r\n",
    "- Mozumdar, A. (2020, February). A guide to encoding categorical features using R. R‑bloggers. https://www.r-bloggers.com/2020/02/a-guide-to-encoding-categorical-features-using-r/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976603c4-f891-4888-9df3-f62ab72fa8b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
